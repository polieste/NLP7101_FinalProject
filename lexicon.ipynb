{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01ab9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('split/train.csv')\n",
    "val_df = pd.read_csv('split/val.csv')\n",
    "test_df = pd.read_csv('split/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e665a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_df = pd.read_csv('NRC-VAD-Lexicon-v2.1/NRC-VAD-Lexicon-v2.1.txt', sep='\\t')\n",
    "lex_df = lex_df.rename(columns={'valence':'v', 'arousal':'a'})[['term','v','a']].dropna()\n",
    "lex_df['term'] = lex_df['term'].str.strip().str.lower()\n",
    "\n",
    "lex_df['v'] = pd.to_numeric(lex_df['v'], errors='coerce').clip(-1, 1)\n",
    "lex_df['a'] = pd.to_numeric(lex_df['a'], errors=\"coerce\").clip(-1, 1)\n",
    "\n",
    "lex_df = lex_df.dropna(subset=['v','a']).groupby('term', as_index=False).mean()\n",
    "lex_df = {r.term:(float(r.v), float(r.a)) for r in lex_df.itertuples(index=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b6dbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_BETA = 0.8\n",
    "EXCL_BUMP = 0.05\n",
    "DECAY = [1.0, 0.95, 0.90]\n",
    "BUT = {'but', 'however', 'though', 'although', 'yet'}\n",
    "NEG = {'not', 'n\\'t', 'never', 'no', 'without', 'none', 'hardly', 'scarcely', 'barely'}\n",
    "INT = {'very', 'really', 'extremely', 'so', 'super', 'absolutely', 'totally', 'too', 'quite', 'highly', 'truly', 'deeply', 'incredibly', 'terribly', 'awfully'}\n",
    "DOWN = {'slightly', 'somewhat', 'rather', 'kinda', 'a bit', 'bit', 'mildly', 'partly'}\n",
    "PUNCT_STOP = {'.', ',', ';', '?', '!', '—', '-', '…', ':', '–'}\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "\n",
    "\n",
    "def norm_elong(s: str) -> str:\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', s)\n",
    "\n",
    "\n",
    "def parse_text(x: str, is_words: bool = False):\n",
    "    x = norm_elong(x or '')\n",
    "    ex = x.count('!')\n",
    "    if is_words:\n",
    "        items = [w.strip().lower() for w in re.split(r'[,\\|;/]+', x) if w.strip()]\n",
    "        toks = [\n",
    "            {'lemma': w, 'pos': 'ADJ', 'txt': w, 'punct': False}\n",
    "            for w in items\n",
    "        ]\n",
    "        return toks, ex\n",
    "    doc = nlp(x)\n",
    "    toks = [\n",
    "        {\n",
    "            'lemma': t.lemma_.lower(),\n",
    "            'pos': t.pos_,\n",
    "            'txt': t.text,\n",
    "            'punct': t.is_punct,\n",
    "        }\n",
    "        for t in doc\n",
    "        if not t.is_space\n",
    "    ]\n",
    "    return toks, ex\n",
    "\n",
    "\n",
    "def build_idf(texts, flags, lex_df):\n",
    "    N = len(texts)\n",
    "    df = defaultdict(int)\n",
    "    for x, iw in zip(texts, flags):\n",
    "        toks, _ = parse_text(x, bool(iw))\n",
    "        seen = {t['lemma'] for t in toks if t['lemma'] in lex_df}\n",
    "        for w in seen:\n",
    "            df[w] += 1\n",
    "    return {\n",
    "        t: math.log((N + 1) / (df.get(t, 0) + 1)) + 1.0\n",
    "        for t in lex_df.keys()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def left_idxs(toks, i: int, k: int = 3):\n",
    "    out = []\n",
    "    j = i - 1\n",
    "    while j >= 0 and len(out) < k:\n",
    "        if toks[j]['punct'] and toks[j]['txt'] in PUNCT_STOP:\n",
    "            break\n",
    "        out.append(j)\n",
    "        j -= 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def adjust_va(v: float, a: float, toks, i: int):\n",
    "    g = 0.0\n",
    "    neg = False\n",
    "    for k, j in enumerate(left_idxs(toks, i, 3)):\n",
    "        w = toks[j]['lemma']\n",
    "        d = DECAY[k] if k < len(DECAY) else DECAY[-1]\n",
    "        if w in INT:\n",
    "            g += 0.3 * d\n",
    "        if w in DOWN:\n",
    "            g += -0.2 * d\n",
    "        if w in NEG:\n",
    "            neg = True\n",
    "    v = (1.0 + g) * v\n",
    "    a = (1.0 + g) * a\n",
    "    if neg:\n",
    "        v = -NEG_BETA * v\n",
    "    v = float(np.clip(v, -2.0, 2.0))\n",
    "    a = float(np.clip(a, -1.0, 1.0))\n",
    "    return v, a\n",
    "\n",
    "\n",
    "def but_idx(toks):\n",
    "    for i, t in enumerate(toks):\n",
    "        if t['lemma'] in BUT:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "def token_weights(toks, idf, lex_df):\n",
    "    wV, wA, vs, as_ = [], [], [], []\n",
    "    for i, t in enumerate(toks):\n",
    "        lem = t['lemma']\n",
    "        if lem not in lex_df:\n",
    "            wV.append(0.0)\n",
    "            wA.append(0.0)\n",
    "            vs.append(0.0)\n",
    "            as_.append(0.0)\n",
    "            continue\n",
    "        v0, a0 = lex_df[lem]\n",
    "        v, a = adjust_va(v0, a0, toks, i)\n",
    "        wb = idf.get(lem, 1.0)\n",
    "        wV.append(wb)\n",
    "        wA.append(wb)\n",
    "        vs.append(v)\n",
    "        as_.append(a)\n",
    "    return (\n",
    "        np.asarray(wV, float),\n",
    "        np.asarray(wA, float),\n",
    "        np.asarray(vs, float),\n",
    "        np.asarray(as_, float),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def aggregate_doc(toks, idf, lex_df):\n",
    "    lem = [t['lemma'] for t in toks if t['lemma'] in lex_df]\n",
    "    if not lem:\n",
    "        return 0.0, 0.0\n",
    "    wV, wA, vs, as_ = token_weights(toks, idf, lex_df)\n",
    "    p = but_idx(toks)\n",
    "    if p is not None:\n",
    "        pre = np.arange(len(toks)) < p\n",
    "        post = ~pre\n",
    "        wV[pre] *= 0.3\n",
    "        wV[post] *= 0.7\n",
    "        wA[pre] *= 0.3\n",
    "        wA[post] *= 0.7\n",
    "    eps = 1e-8\n",
    "    V = float(np.nan_to_num(np.nansum(wV * vs) / (np.nansum(wV) + eps)))\n",
    "    A = float(np.nan_to_num(np.nansum(wA * as_) / (np.nansum(wA) + eps)))\n",
    "    V = float(np.clip(V, -2.0, 2.0))\n",
    "    A = float(np.clip(A, -1.0, 1.0))\n",
    "    return V, A\n",
    "\n",
    "\n",
    "def bump_excl(A: float, ex: int):\n",
    "    return float(\n",
    "        np.clip(A + EXCL_BUMP * min(3, int(ex)), -1.0, 1.0)\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_idf(train_df: pd.DataFrame, lex_df):\n",
    "    return build_idf(\n",
    "        train_df['text'].tolist(),\n",
    "        train_df['is_words'].tolist(),\n",
    "        lex_df,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def score_df(df: pd.DataFrame, idf, lex_df):\n",
    "    VV, AA = [], []\n",
    "    for x, iw in zip(df['text'], df['is_words']):\n",
    "        toks, ex = parse_text(str(x), bool(iw))\n",
    "        v, a = aggregate_doc(toks, idf, lex_df)\n",
    "        a = bump_excl(a, ex)\n",
    "        VV.append(v)\n",
    "        AA.append(a)\n",
    "    out = df.copy()\n",
    "    out['V'] = VV\n",
    "    out['A'] = AA\n",
    "    return out\n",
    "\n",
    "\n",
    "def fit_lin(y_true, y_pred):\n",
    "    x = np.asarray(y_pred)\n",
    "    y = np.asarray(y_true)\n",
    "    x = np.c_[np.ones_like(x), x]\n",
    "    w, *_ = np.linalg.lstsq(x, y, rcond=None)\n",
    "    return float(w[0]), float(w[1])\n",
    "\n",
    "\n",
    "def apply_lin(x, w0: float, w1: float, lo: float, hi: float):\n",
    "    return np.clip(w0 + w1 * np.asarray(x), lo, hi)\n",
    "\n",
    "\n",
    "def apply_global(dfsc: pd.DataFrame, w0v: float, w1v: float, w0a: float, w1a: float):\n",
    "    dfsc = dfsc.copy()\n",
    "    dfsc['V_g'] = apply_lin(dfsc['V'], w0v, w1v, -2.0, 2.0)\n",
    "    dfsc['A_g'] = apply_lin(dfsc['A'], w0a, w1a, -1.0, 1.0)\n",
    "    return dfsc\n",
    "\n",
    "\n",
    "def fit_local(train_g: pd.DataFrame, min_user_n: int = 6):\n",
    "    cal_v = {}\n",
    "    cal_a = {}\n",
    "    for uid, g in train_g.groupby('user_id'):\n",
    "        if len(g) >= min_user_n:\n",
    "            cal_v[uid] = fit_lin(g['valence'], g['V_g'])\n",
    "            cal_a[uid] = fit_lin(g['arousal'], g['A_g'])\n",
    "    return cal_v, cal_a\n",
    "\n",
    "\n",
    "def apply_local(dfsc: pd.DataFrame, cal_v, cal_a):\n",
    "    dfsc = dfsc.copy()\n",
    "    Vh, Ah = [], []\n",
    "    for r in dfsc.itertuples(index=False):\n",
    "        uid = getattr(r, 'user_id')\n",
    "        v_in = getattr(r, 'V_g')\n",
    "        a_in = getattr(r, 'A_g')\n",
    "        if uid in cal_v and uid in cal_a:\n",
    "            w0, w1 = cal_v[uid]\n",
    "            vv = apply_lin(v_in, w0, w1, -2.0, 2.0)\n",
    "            w0, w1 = cal_a[uid]\n",
    "            aa = apply_lin(a_in, w0, w1, -1.0, 1.0)\n",
    "        else:\n",
    "            vv, aa = v_in, a_in\n",
    "        Vh.append(float(vv))\n",
    "        Ah.append(float(aa))\n",
    "    dfsc['V_local'] = Vh\n",
    "    dfsc['A_local'] = Ah\n",
    "    return dfsc\n",
    "\n",
    "\n",
    "def eval(df: pd.DataFrame, v_col_pred: str, a_col_pred: str):\n",
    "    dv = df['valence'] - df[v_col_pred]\n",
    "    da = df['arousal'] - df[a_col_pred]\n",
    "    out = {\n",
    "        'V_MAE': float(np.mean(np.abs(dv))),\n",
    "        'V_RMSE': float(np.sqrt(np.mean(dv**2))),\n",
    "        'A_MAE': float(np.mean(np.abs(da))),\n",
    "        'A_RMSE': float(np.sqrt(np.mean(da**2))),\n",
    "    }\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c96cc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'V_MAE': 0.8545317522903663, 'V_RMSE': 1.0674621103034159, 'A_MAE': 0.558384987161822, 'A_RMSE': 0.6760707365269816}\n"
     ]
    }
   ],
   "source": [
    "idf = fit_idf(train_df, lex_df)\n",
    "\n",
    "train_s = score_df(train_df, idf, lex_df)\n",
    "val_s = score_df(val_df, idf, lex_df)\n",
    "test_s = score_df(test_df, idf, lex_df)\n",
    "\n",
    "w0v, w1v = fit_lin(train_df['valence'], train_s['V'])\n",
    "w0a, w1a = fit_lin(train_df['arousal'], train_s['A'])\n",
    "train_g = apply_global(train_s, w0v, w1v, w0a, w1a)\n",
    "val_g = apply_global(val_s, w0v, w1v, w0a, w1a)\n",
    "test_g = apply_global(test_s, w0v, w1v, w0a, w1a)\n",
    "cal_v, cal_a = fit_local(train_g, min_user_n=6)\n",
    "val_final = apply_local(val_g, cal_v, cal_a)\n",
    "test_final = apply_local(test_g, cal_v, cal_a)\n",
    "\n",
    "res_val = eval(val_final, 'V_local', 'A_local')\n",
    "res_test = eval(test_final, 'V_local', 'A_local')\n",
    "\n",
    "print(res_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-asm1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
