{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d2af373",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('TRAIN_RELEASE_3SEP2025/train_subtask1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b227108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df, seed=11, u_val=0.10, u_test=0.15, tr=0.75, vr=(0.4,0.6), ms=3, mt=1):\n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values(['user_id','timestamp']).reset_index(drop=True)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    users = df['user_id'].unique()\n",
    "    tiny = set(df.groupby('user_id').size()[lambda s: s < ms].index)\n",
    "\n",
    "    n_users = len(users)\n",
    "    n_val = max(1, int(round(n_users * u_val)))\n",
    "    n_tst = max(1, int(round(n_users * u_test)))\n",
    "\n",
    "    cand = [u for u in users if u not in tiny]\n",
    "    rng.shuffle(cand)\n",
    "    pool = list(tiny) + cand\n",
    "    u_val_set  = set(pool[:n_val])\n",
    "    u_test_set = set(pool[n_val:n_val+n_tst])\n",
    "\n",
    "    def split_user(g):\n",
    "        uid = g['user_id'].iloc[0]\n",
    "        if uid in u_val_set:\n",
    "            g['split'] = 'val_unseen'\n",
    "            return g\n",
    "        if uid in u_test_set:\n",
    "            g['split'] = 'test_unseen'\n",
    "            return g\n",
    "        n = len(g)\n",
    "        tr_end = max(1, int(np.floor(n * tr)))\n",
    "        R = n - tr_end\n",
    "        p = float(rng.uniform(vr[0], vr[1]))\n",
    "        val_end = tr_end + int(np.floor(R * p))\n",
    "        if n - val_end < mt:\n",
    "            val_end = max(tr_end, n - mt)\n",
    "        g['split'] = (['train']*tr_end +\n",
    "                      ['val_seen']*max(0, val_end - tr_end) +\n",
    "                      ['test_seen']*(n - val_end))\n",
    "        return g\n",
    "\n",
    "    df = df.groupby('user_id', group_keys=False).apply(split_user)\n",
    "    df['group'] = np.where(df['split'].isin(['val_unseen','test_unseen']), 'unseen', 'seen')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f1671c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3278/3693774904.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('user_id', group_keys=False).apply(split_user)\n"
     ]
    }
   ],
   "source": [
    "df = data_split(df, seed=11)\n",
    "\n",
    "train = df[df.split == 'train']\n",
    "val   = df[df.split.isin(['val_seen','val_unseen'])]\n",
    "test  = df[df.split.isin(['test_seen','test_unseen'])]\n",
    "\n",
    "train.to_csv(\"split/train.csv\", index=False)\n",
    "val.to_csv(\"split/val.csv\", index=False)\n",
    "test.to_csv(\"split/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77dc2cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== rows_per_split ==\n",
      "             rows\n",
      "split            \n",
      "train        1529\n",
      "test_unseen   539\n",
      "test_seen     332\n",
      "val_seen      229\n",
      "val_unseen    135\n",
      "\n",
      "== users_per_split ==\n",
      "             users\n",
      "split             \n",
      "test_seen      102\n",
      "test_unseen     21\n",
      "train          102\n",
      "val_seen        83\n",
      "val_unseen      14\n",
      "\n",
      "== users_seen_unseen_per_split ==\n",
      "         split   group  users\n",
      "0    test_seen    seen    102\n",
      "1  test_unseen  unseen     21\n",
      "2        train    seen    102\n",
      "3     val_seen    seen     83\n",
      "4   val_unseen  unseen     14\n",
      "\n",
      "== rows_per_set ==\n",
      "       rows\n",
      "set        \n",
      "train  1529\n",
      "test    871\n",
      "val     364\n",
      "\n",
      "== users_per_set ==\n",
      "       users\n",
      "set         \n",
      "test     123\n",
      "train    102\n",
      "val       97\n",
      "\n",
      "== users_seen_unseen_per_set ==\n",
      "       seen  unseen\n",
      "set                \n",
      "test    102      21\n",
      "train   102       0\n",
      "val      83      14\n",
      "\n",
      "== is_words_counts_split ==\n",
      "is_words     False  True \n",
      "split                    \n",
      "test_seen      169    163\n",
      "test_unseen    306    233\n",
      "train          687    842\n",
      "val_seen       115    114\n",
      "val_unseen      54     81\n",
      "\n",
      "== is_words_pct_split ==\n",
      "is_words     False  True \n",
      "split                    \n",
      "test_seen    0.509  0.491\n",
      "test_unseen  0.568  0.432\n",
      "train        0.449  0.551\n",
      "val_seen     0.502  0.498\n",
      "val_unseen   0.400  0.600\n",
      "\n",
      "== is_words_counts_set ==\n",
      "is_words  False  True \n",
      "set                   \n",
      "test        475    396\n",
      "train       687    842\n",
      "val         169    195\n",
      "\n",
      "== is_words_pct_set ==\n",
      "is_words  False  True \n",
      "set                   \n",
      "test      0.545  0.455\n",
      "train     0.449  0.551\n",
      "val       0.464  0.536\n",
      "\n",
      "== is_words_counts_group_split ==\n",
      "is_words            False  True \n",
      "group  split                    \n",
      "seen   test_seen      169    163\n",
      "       train          687    842\n",
      "       val_seen       115    114\n",
      "unseen test_unseen    306    233\n",
      "       val_unseen      54     81\n",
      "\n",
      "== is_words_pct_group_split ==\n",
      "is_words            False  True \n",
      "group  split                    \n",
      "seen   test_seen    0.509  0.491\n",
      "       train        0.449  0.551\n",
      "       val_seen     0.502  0.498\n",
      "unseen test_unseen  0.568  0.432\n",
      "       val_unseen   0.400  0.600\n",
      "\n",
      "== is_words_counts_group_set ==\n",
      "is_words      False  True \n",
      "group  set                \n",
      "seen   test     169    163\n",
      "       train    687    842\n",
      "       val      115    114\n",
      "unseen test     306    233\n",
      "       val       54     81\n",
      "\n",
      "== is_words_pct_group_set ==\n",
      "is_words      False  True \n",
      "group  set                \n",
      "seen   test   0.509  0.491\n",
      "       train  0.449  0.551\n",
      "       val    0.502  0.498\n",
      "unseen test   0.568  0.432\n",
      "       val    0.400  0.600\n"
     ]
    }
   ],
   "source": [
    "def analyze(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    df['set'] = df['split'].map(lambda s: 'train' if s=='train' else ('val' if 'val_' in s else 'test'))\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    out['rows_per_split']  = df['split'].value_counts().rename('rows').to_frame()\n",
    "    out['users_per_split'] = df.groupby('split')['user_id'].nunique().rename('users').to_frame()\n",
    "    out['users_seen_unseen_per_split'] = (\n",
    "        df.groupby(['split','group'])['user_id'].nunique().rename('users').reset_index()\n",
    "    )\n",
    "\n",
    "    out['rows_per_set']  = df['set'].value_counts().rename('rows').to_frame()\n",
    "    out['users_per_set'] = df.groupby('set')['user_id'].nunique().rename('users').to_frame()\n",
    "    out['users_seen_unseen_per_set'] = (\n",
    "        df.groupby(['set','group'])['user_id'].nunique().unstack('group', fill_value=0)\n",
    "        .rename_axis(None, axis=1)\n",
    "    )\n",
    "\n",
    "    iw_split = df.pivot_table(index='split', columns='is_words', values='text_id', aggfunc='count', fill_value=0)\n",
    "    iw_set   = df.pivot_table(index='set',   columns='is_words', values='text_id', aggfunc='count', fill_value=0)\n",
    "    out['is_words_counts_split'] = iw_split\n",
    "    out['is_words_pct_split']    = (iw_split.div(iw_split.sum(axis=1), axis=0)).round(3)\n",
    "    out['is_words_counts_set']   = iw_set\n",
    "    out['is_words_pct_set']      = (iw_set.div(iw_set.sum(axis=1), axis=0)).round(3)\n",
    "\n",
    "    iw_gs = df.pivot_table(index=['group','split'], columns='is_words', values='text_id', aggfunc='count', fill_value=0)\n",
    "    iw_gS = df.pivot_table(index=['group','set'],   columns='is_words', values='text_id', aggfunc='count', fill_value=0)\n",
    "    out['is_words_counts_group_split'] = iw_gs\n",
    "    out['is_words_pct_group_split']    = (iw_gs.div(iw_gs.sum(axis=1), axis=0)).round(3)\n",
    "    out['is_words_counts_group_set']   = iw_gS\n",
    "    out['is_words_pct_group_set']      = (iw_gS.div(iw_gS.sum(axis=1), axis=0)).round(3)\n",
    "\n",
    "    for k,v in out.items():\n",
    "        print(f\"\\n== {k} ==\")\n",
    "        print(v)\n",
    "\n",
    "    return out\n",
    "\n",
    "res = analyze(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_BETA = 0.8\n",
    "DECAY = [1.0, 0.95, 0.90]\n",
    "# POS_W  = {'ADJ':1.4,'VERB':1.2,'NOUN':1.0,'ADV':1.0}\n",
    "# POS_DEF = 0.5\n",
    "BUT = {'but','however','though','although','yet'}\n",
    "NEG = {'not',\"n't\",'never','no','without','none','hardly','scarcely','barely'}\n",
    "INT  = {'very','really','extremely','so','super','absolutely','totally','too','quite','highly','truly','deeply','incredibly','terribly','awfully'}\n",
    "DOWN = {'slightly','somewhat','rather','kinda','a bit','bit','mildly','partly'}\n",
    "EXCL_BUMP = 0.05\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "PUNCT_STOP = {'.',',',';','?','!','—','-','…',':','–'}\n",
    "\n",
    "def _norm_elong(s): return re.sub(r'(.)\\1{2,}', r'\\1\\1', s)\n",
    "\n",
    "def load_lexicon(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\", engine=\"python\")\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    df = df.rename(columns={\"valence\":\"v\",\"arousal\":\"a\"})[[\"term\",\"v\",\"a\"]].dropna()\n",
    "    df[\"term\"] = df[\"term\"].str.strip().str.lower()\n",
    "    df[\"v\"] = pd.to_numeric(df[\"v\"], errors=\"coerce\").clip(-1, 1)\n",
    "    df[\"a\"] = pd.to_numeric(df[\"a\"], errors=\"coerce\").clip(-1, 1)\n",
    "    df = df.dropna(subset=[\"v\",\"a\"]).groupby(\"term\", as_index=False).mean()\n",
    "    return {r.term:(float(r.v), float(r.a)) for r in df.itertuples(index=False)}\n",
    "\n",
    "def parse_text(x, is_words=False):\n",
    "    x = _norm_elong(x or \"\")\n",
    "    ex = x.count('!')\n",
    "    if is_words:\n",
    "        items = [w.strip().lower() for w in re.split(r'[,\\|;/]+', x) if w.strip()]\n",
    "        toks = [{'lemma':w, 'pos':'ADJ', 'txt':w, 'punct':False} for w in items]\n",
    "        return toks, ex\n",
    "    doc = nlp(x)\n",
    "    toks = [{'lemma':t.lemma_.lower(), 'pos':t.pos_, 'txt':t.text, 'punct':t.is_punct} for t in doc if not t.is_space]\n",
    "    return toks, ex\n",
    "\n",
    "def build_idf(texts, flags, LEX):\n",
    "    N = len(texts); df = defaultdict(int)\n",
    "    for x, iw in zip(texts, flags):\n",
    "        toks, _ = parse_text(x, bool(iw))\n",
    "        seen = set(t['lemma'] for t in toks if t['lemma'] in LEX)\n",
    "        for w in seen: df[w] += 1\n",
    "    return {t: math.log((N+1)/ (df.get(t,0)+1)) + 1.0 for t in LEX.keys()}\n",
    "\n",
    "def _left_idxs(toks, i, k=3):\n",
    "    out, j = [], i-1\n",
    "    while j >= 0 and len(out) < k:\n",
    "        if toks[j]['punct'] and toks[j]['txt'] in PUNCT_STOP: break\n",
    "        out.append(j); j -= 1\n",
    "    return out\n",
    "\n",
    "def adjust_va(v, a, toks, i):\n",
    "    g, neg = 0.0, False\n",
    "    for k, j in enumerate(_left_idxs(toks, i, 3)):\n",
    "        w = toks[j]['lemma']; d = DECAY[k] if k < len(DECAY) else DECAY[-1]\n",
    "        if w in INT:  g += 0.3 * d\n",
    "        if w in DOWN: g += (-0.2) * d\n",
    "        if w in NEG:  neg = True\n",
    "    v = (1.0 + g) * v\n",
    "    a = (1.0 + g) * a\n",
    "    if neg: v = -NEG_BETA * v\n",
    "    return float(np.clip(v, -2.0, 2.0)), float(np.clip(a, -1.0, 1.0))\n",
    "\n",
    "def _but_idx(toks):\n",
    "    for i,t in enumerate(toks):\n",
    "        if t['lemma'] in BUT: return i\n",
    "    return None\n",
    "\n",
    "def _weights(toks, tf, idf, LEX):\n",
    "    wV, wA, vs, as_ = [], [], [], []\n",
    "    for i, t in enumerate(toks):\n",
    "        lem, pos = t['lemma'], t['pos']\n",
    "        if lem not in LEX:\n",
    "            wV.append(0.0); wA.append(0.0); vs.append(0.0); as_.append(0.0)\n",
    "            continue\n",
    "        v0, a0 = LEX[lem]\n",
    "        v, a = adjust_va(v0, a0, toks, i)\n",
    "        wb = idf.get(lem, 1.0)\n",
    "        wV.append(wb * (1 + 0.3 * abs(a)))\n",
    "        wA.append(wb)\n",
    "        vs.append(v); as_.append(a)\n",
    "    return np.asarray(wV, float), np.asarray(wA, float), np.asarray(vs, float), np.asarray(as_, float)\n",
    "\n",
    "def aggregate_doc(toks, idf, LEX):\n",
    "    lem = [t['lemma'] for t in toks if t['lemma'] in LEX]\n",
    "    if not lem: return 0.0, 0.0\n",
    "    tf = Counter(lem)\n",
    "    wV, wA, vs, as_ = _weights(toks, tf, idf, LEX)\n",
    "    p = _but_idx(toks)\n",
    "    if p is not None:\n",
    "        pre = np.arange(len(toks)) < p\n",
    "        post = ~pre\n",
    "        wV[pre] *= 0.3; wV[post] *= 0.7\n",
    "        wA[pre] *= 0.3; wA[post] *= 0.7\n",
    "    eps = 1e-8\n",
    "    V = float(np.nan_to_num(np.nansum(wV*vs) / (np.nansum(wV)+eps)))\n",
    "    A = float(np.nan_to_num(np.nansum(wA*as_) / (np.nansum(wA)+eps)))\n",
    "    V = float(np.clip(V, -2.0, 2.0))\n",
    "    A = float(np.clip(A, -1.0, 1.0))\n",
    "    return V, A\n",
    "\n",
    "def bump_excl(A, ex): \n",
    "    return float(np.clip(A + EXCL_BUMP*min(3, int(ex)), -1.0, 1.0))\n",
    "\n",
    "def fit_idf_from_train(train_df, LEX):\n",
    "    return build_idf(train_df['text'].tolist(), train_df['is_words'].tolist(), LEX)\n",
    "\n",
    "def score_df(df, idf, LEX):\n",
    "    VV, AA = [], []\n",
    "    for x, iw in zip(df['text'], df['is_words']):\n",
    "        toks, ex = parse_text(str(x), bool(iw))\n",
    "        v, a = aggregate_doc(toks, idf, LEX)\n",
    "        a = bump_excl(a, ex)\n",
    "        VV.append(v); AA.append(a)\n",
    "    out = df.copy()\n",
    "    out['V'], out['A'] = VV, AA\n",
    "    return out\n",
    "\n",
    "def fit_lin(y_true, y_pred):\n",
    "    x = np.asarray(y_pred); y = np.asarray(y_true)\n",
    "    x = np.c_[np.ones_like(x), x]\n",
    "    w, *_ = np.linalg.lstsq(x, y, rcond=None)\n",
    "    return float(w[0]), float(w[1])\n",
    "\n",
    "def apply_lin(x, w0, w1, lo, hi):\n",
    "    return np.clip(w0 + w1*np.asarray(x), lo, hi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2cff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX = load_lexicon(\"NRC-VAD-Lexicon-v2.1/NRC-VAD-Lexicon-v2.1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6dbf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VAL — no_cal ===\n",
      "         V_MAE  V_RMSE     V_r   A_MAE  A_RMSE     A_r      n  users\n",
      "ALL     0.8763  1.0789  0.4950  0.9117  1.1357  0.2972  364.0   97.0\n",
      "SEEN    0.8181  1.0229  0.5877  1.0393  1.2385  0.3675  229.0   83.0\n",
      "UNSEEN  0.9751  1.1678  0.3477  0.6953  0.9359  0.2343  135.0   14.0\n",
      "\n",
      "=== VAL — global ===\n",
      "         V_MAE  V_RMSE     V_r   A_MAE  A_RMSE     A_r      n  users\n",
      "ALL     0.8537  1.0555  0.4950  0.6057  0.7205  0.3273  364.0   97.0\n",
      "SEEN    0.7797  0.9632  0.5877  0.5914  0.7220  0.4021  229.0   83.0\n",
      "UNSEEN  0.9792  1.1958  0.3477  0.6300  0.7179  0.2630  135.0   14.0\n",
      "\n",
      "=== VAL — user ===\n",
      "         V_MAE  V_RMSE     V_r   A_MAE  A_RMSE     A_r      n  users\n",
      "ALL     0.7910  1.0331  0.5315  0.6188  0.8158  0.4208  364.0   97.0\n",
      "SEEN    0.6826  0.9448  0.6247  0.5736  0.7359  0.4348  229.0   83.0\n",
      "UNSEEN  0.9751  1.1678  0.3477  0.6953  0.9359  0.2343  135.0   14.0\n",
      "\n",
      "=== VAL — glob+user ===\n",
      "         V_MAE  V_RMSE     V_r   A_MAE  A_RMSE     A_r      n  users\n",
      "ALL     0.7875  1.0401  0.5364  0.5846  0.7138  0.3647  364.0   97.0\n",
      "SEEN    0.6745  0.9362  0.6332  0.5578  0.7114  0.4686  229.0   83.0\n",
      "UNSEEN  0.9792  1.1958  0.3477  0.6300  0.7179  0.2630  135.0   14.0\n",
      "\n",
      "=== TEST — no_cal ===\n",
      "         V_MAE  V_RMSE     V_r   A_MAE  A_RMSE     A_r      n  users\n",
      "ALL     0.9572  1.1518  0.5783  0.8150  1.0426  0.2620  871.0  123.0\n",
      "SEEN    0.9536  1.1389  0.6346  0.8662  1.0976  0.3676  332.0  102.0\n",
      "UNSEEN  0.9594  1.1597  0.5219  0.7834  1.0073  0.1793  539.0   21.0\n",
      "\n",
      "=== TEST — global ===\n",
      "         V_MAE  V_RMSE     V_r   A_MAE  A_RMSE     A_r      n  users\n",
      "ALL     0.8945  1.0863  0.5783  0.5842  0.6899  0.2464  871.0  123.0\n",
      "SEEN    0.8570  1.0415  0.6346  0.6107  0.7208  0.3529  332.0  102.0\n",
      "UNSEEN  0.9176  1.1131  0.5219  0.5679  0.6701  0.1663  539.0   21.0\n",
      "\n",
      "=== TEST — user ===\n",
      "         V_MAE  V_RMSE     V_r   A_MAE  A_RMSE     A_r      n  users\n",
      "ALL     0.8810  1.0981  0.5686  0.7034  0.9135  0.2343  871.0  123.0\n",
      "SEEN    0.7538  0.9901  0.6679  0.5734  0.7361  0.3727  332.0  102.0\n",
      "UNSEEN  0.9594  1.1597  0.5219  0.7834  1.0073  0.1793  539.0   21.0\n",
      "\n",
      "=== TEST — glob+user ===\n",
      "         V_MAE  V_RMSE     V_r   A_MAE  A_RMSE     A_r      n  users\n",
      "ALL     0.8517  1.0641  0.5976  0.5584  0.6761  0.3092  871.0  123.0\n",
      "SEEN    0.7446  0.9795  0.6765  0.5429  0.6856  0.4538  332.0  102.0\n",
      "UNSEEN  0.9176  1.1131  0.5219  0.5679  0.6701  0.1663  539.0   21.0\n"
     ]
    }
   ],
   "source": [
    "def metrics(df, v_col_pred, a_col_pred, v_col_true='valence', a_col_true='arousal'):\n",
    "    dv = df[v_col_true] - df[v_col_pred]\n",
    "    da = df[a_col_true] - df[a_col_pred]\n",
    "    out = {\n",
    "        'V_MAE': float(np.mean(np.abs(dv))),\n",
    "        'V_RMSE': float(np.sqrt(np.mean(dv**2))),\n",
    "        'A_MAE': float(np.mean(np.abs(da))),\n",
    "        'A_RMSE': float(np.sqrt(np.mean(da**2))),\n",
    "        'n': int(len(df)),\n",
    "        'users': int(df['user_id'].nunique())\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def metrics_by_group(df, v_col_pred, a_col_pred):\n",
    "    all_ = metrics(df, v_col_pred, a_col_pred)\n",
    "    res = {'ALL': all_}\n",
    "    for g, gdf in df.groupby('group'):\n",
    "        res[g.upper()] = metrics(gdf, v_col_pred, a_col_pred)\n",
    "    return pd.DataFrame(res).T\n",
    "\n",
    "def run_all_approaches(train, val, test, LEX, min_user_n=6):\n",
    "    idf = fit_idf_from_train(train, LEX)\n",
    "\n",
    "    train_s = score_df(train, idf, LEX)\n",
    "    val_s   = score_df(val,   idf, LEX)\n",
    "    test_s  = score_df(test,  idf, LEX)\n",
    "\n",
    "    val_nc  = val_s.rename(columns={'V':'V_nc','A':'A_nc'})\n",
    "    test_nc = test_s.rename(columns={'V':'V_nc','A':'A_nc'})\n",
    "\n",
    "    w0v, w1v = fit_lin(train['valence'], train_s['V'])\n",
    "    w0a, w1a = fit_lin(train['arousal'], train_s['A'])\n",
    "\n",
    "    def apply_global(dfsc):\n",
    "        dfsc = dfsc.copy()\n",
    "        dfsc['V_g'] = apply_lin(dfsc['V'], w0v, w1v, -2,  2)\n",
    "        dfsc['A_g'] = apply_lin(dfsc['A'], w0a, w1a, -1,  1)\n",
    "        return dfsc\n",
    "\n",
    "    train_g = apply_global(train_s)\n",
    "    val_g   = apply_global(val_s)\n",
    "    test_g  = apply_global(test_s)\n",
    "\n",
    "    cal_v_u, cal_a_u = {}, {}\n",
    "    for uid, g in train_s.groupby('user_id'):\n",
    "        if len(g) >= min_user_n:\n",
    "            cal_v_u[uid] = fit_lin(g['valence'], g['V'])\n",
    "            cal_a_u[uid] = fit_lin(g['arousal'], g['A'])\n",
    "\n",
    "    def apply_user(dfsc, src_V='V', src_A='A', outV='V_u', outA='A_u', fallback=('raw')):\n",
    "        dfsc = dfsc.copy()\n",
    "        Vh, Ah = [], []\n",
    "        for r in dfsc.itertuples(index=False):\n",
    "            uid = getattr(r, 'user_id')\n",
    "            v_in = getattr(r, src_V); a_in = getattr(r, src_A)\n",
    "            if uid in cal_v_u and uid in cal_a_u:\n",
    "                w0,w1 = cal_v_u[uid]; vv = apply_lin(v_in, w0, w1, -2, 2)\n",
    "                w0,w1 = cal_a_u[uid]; aa = apply_lin(a_in, w0, w1, -1, 1)\n",
    "            else:\n",
    "                if fallback == 'global' and ('V_g' in dfsc and 'A_g' in dfsc):\n",
    "                    vv = getattr(r, 'V_g'); aa = getattr(r, 'A_g')\n",
    "                else:\n",
    "                    vv = v_in; aa = a_in\n",
    "            Vh.append(vv); Ah.append(aa)\n",
    "        dfsc[outV] = Vh; dfsc[outA] = Ah\n",
    "        return dfsc\n",
    "\n",
    "    val_u  = apply_user(val_s,  src_V='V',   src_A='A',   outV='V_u',  outA='A_u',  fallback='raw')\n",
    "    test_u = apply_user(test_s, src_V='V',   src_A='A',   outV='V_u',  outA='A_u',  fallback='raw')\n",
    "\n",
    "    cal_v_gu, cal_a_gu = {}, {}\n",
    "    for uid, g in train_g.groupby('user_id'):\n",
    "        if len(g) >= min_user_n:\n",
    "            cal_v_gu[uid] = fit_lin(g['valence'], g['V_g'])\n",
    "            cal_a_gu[uid] = fit_lin(g['arousal'], g['A_g'])\n",
    "\n",
    "    def apply_global_user(dfsc):\n",
    "        dfsc = dfsc.copy()\n",
    "        if 'V_g' not in dfsc: dfsc = apply_global(dfsc)\n",
    "        Vh, Ah = [], []\n",
    "        for r in dfsc.itertuples(index=False):\n",
    "            uid = getattr(r, 'user_id')\n",
    "            v_in = getattr(r, 'V_g'); a_in = getattr(r, 'A_g')\n",
    "            if uid in cal_v_gu and uid in cal_a_gu:\n",
    "                w0,w1 = cal_v_gu[uid]; vv = apply_lin(v_in, w0, w1, -2, 2)\n",
    "                w0,w1 = cal_a_gu[uid]; aa = apply_lin(a_in, w0, w1, -1, 1)\n",
    "            else:\n",
    "                vv, aa = v_in, a_in\n",
    "            Vh.append(vv); Ah.append(aa)\n",
    "        dfsc['V_gu'] = Vh; dfsc['A_gu'] = Ah\n",
    "        return dfsc\n",
    "\n",
    "    val_gu  = apply_global_user(val_s)\n",
    "    test_gu = apply_global_user(test_s)\n",
    "\n",
    "    rep = {}\n",
    "\n",
    "    rep[('val','no_cal')]    = metrics_by_group(val_nc,  'V_nc','A_nc')\n",
    "    rep[('val','global')]    = metrics_by_group(val_g,   'V_g', 'A_g')\n",
    "    rep[('val','user')]      = metrics_by_group(val_u,   'V_u', 'A_u')\n",
    "    rep[('val','glob+user')] = metrics_by_group(val_gu,  'V_gu','A_gu')\n",
    "\n",
    "    rep[('test','no_cal')]    = metrics_by_group(test_nc,  'V_nc','A_nc')\n",
    "    rep[('test','global')]    = metrics_by_group(test_g,   'V_g', 'A_g')\n",
    "    rep[('test','user')]      = metrics_by_group(test_u,   'V_u', 'A_u')\n",
    "    rep[('test','glob+user')] = metrics_by_group(test_gu,  'V_gu','A_gu')\n",
    "\n",
    "    for (split, name), dfm in rep.items():\n",
    "        print(f\"\\n=== {split.upper()} — {name} ===\")\n",
    "        print(dfm[['V_MAE','V_RMSE','V_r','A_MAE','A_RMSE','A_r','n','users']].round(4).to_string())\n",
    "\n",
    "    scored = {\n",
    "        'train_raw': train_s,\n",
    "        'val_no_cal': val_nc, 'test_no_cal': test_nc,\n",
    "        'val_global': val_g,  'test_global': test_g,\n",
    "        'val_user': val_u,    'test_user': test_u,\n",
    "        'val_glob_user': val_gu, 'test_glob_user': test_gu\n",
    "    }\n",
    "    return rep, scored\n",
    "\n",
    "idf = fit_idf_from_train(train, LEX)\n",
    "train_s = score_df(train, idf, LEX)\n",
    "val_s   = score_df(val,   idf, LEX)\n",
    "test_s  = score_df(test,  idf, LEX)\n",
    "\n",
    "rep, scored = run_all_approaches(train, val, test, LEX, min_user_n=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-asm1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
